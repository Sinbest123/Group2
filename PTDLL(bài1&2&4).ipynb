{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 1)**"
      ],
      "metadata": {
        "id": "mq9KrJrR97PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**People You Might Know**\n",
        "\n",
        "This a Spark program that implements a simple “People You Might Know” social network friendship recommendation algorithm. The key idea is that if two people have a lot of mutual friends, then the system should recommend that they connect with each other.\n",
        "\n",
        "We use the resilient distributed dataset (RDD).\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "Let us use a simple algorithm such that, for each user U, the algorithm rec- ommends N = 10 users who are not already friends with U, but have the most number of mutual friends in common with U.\n",
        "\n",
        "**Requirement:**\n",
        "\n",
        "PySpark 3.1.2"
      ],
      "metadata": {
        "id": "PgbNeJl-phOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "skf3YcwBqKBw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "38UXP4Z4rIwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Avoids scroll-in-the-scroll in the entire Notebook\n",
        "from IPython.display import Javascript\n",
        "def resize_colab_cell():\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 400})'))\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MFwxXqTrehf",
        "outputId": "c9000645-a509-43c8-c37d-ad5cfaf784e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bDY_z-Evr4TM",
        "outputId": "b62c7ae1-31e4-4ca2-f267-f3176225e4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Yqe54ocjsCPM",
        "outputId": "8f10a830-ccd0-448c-a5e6-f6de73a89b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-30 13:28:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  2.67MB/s    in 12s     \n",
            "\n",
            "2023-03-30 13:28:30 (1.08 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def line_to_friend_ownership(line):\n",
        "    split = line.split()\n",
        "    user_id = int(split[0])\n",
        "    if len(split) == 1:\n",
        "        friends = []\n",
        "    else:\n",
        "        friends = list(map(lambda x: int(x), split[1].split(',')))\n",
        "    return user_id, friends\n",
        "\n",
        "def friend_ownership_to_connection(f_o):\n",
        "    user_id = f_o[0]\n",
        "    friends = f_o[1]\n",
        "    connections = []\n",
        "    for friend_id in friends:\n",
        "        key = (user_id, friend_id)\n",
        "        if user_id > friend_id:\n",
        "            key = (friend_id, user_id)\n",
        "        connections.append((key, 0))  # they are friends, value=0\n",
        "    for friend_pair in itertools.combinations(friends, 2):\n",
        "        friend_0 = friend_pair[0]\n",
        "        friend_1 = friend_pair[1]\n",
        "        key = (friend_0, friend_1)\n",
        "        if friend_0 > friend_1:\n",
        "            key = (friend_1, friend_0)\n",
        "        connections.append((key, 1))  # they have mutual friends, value=1\n",
        "    return connections\n",
        "    \n",
        "def mutual_friend_count_to_recommendation(f):\n",
        "    pair = f[0]\n",
        "    friend0 = pair[0]\n",
        "    friend1 = pair[1]\n",
        "    noMutFriends = f[1]\n",
        "    rec0 = (friend0, (friend1, noMutFriends))\n",
        "    rec1 = (friend1, (friend0, noMutFriends))\n",
        "    return [rec0, rec1]\n",
        "\n",
        "def recommendation_to_sorted_truncated(recs):\n",
        "    recs.sort(key=lambda x: (-x[1], x[0]))\n",
        "    return list(map(lambda x: x[0], recs))[:10]\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rvI1GjZ_tLhS",
        "outputId": "2617d686-ac24-4b21-ad87-6939cd186e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from text file\n",
        "lines = sc.textFile(\"/content/soc-LiveJournal1Adj.txt\")\n",
        "\n",
        "# Map each line to the form: (user_id, [friend_id_0, friend_id_1, ...])\n",
        "friend_ownership = lines.map(line_to_friend_ownership).filter(lambda friend: '' != friend[1])#.filter(lambda friend: 1000> friend[0]) #take 1000 samples for testing\n",
        "\n",
        "# Map friend ownerships to instances of ((user_id, friend_id), VALUE).\n",
        "# VALUE = 0 => pairs are already friends.\n",
        "# VALUE = 1 => pairs have mutual friends.\n",
        "friend_edges = friend_ownership.flatMap(friend_ownership_to_connection)\n",
        "friend_edges.cache()\n",
        "\n",
        "# Filter pairs that are already friends\n",
        "mutual_friend = friend_edges.groupByKey() \\\n",
        "    .filter(lambda edge: 0 not in edge[1]) \\\n",
        "    .flatMap(lambda x: [(x[0],item) for item in x[1]]) # flat it to count total mutual friends No; use map directly causes bugs\n",
        "\n",
        "# Count mutual friends by adding up values\n",
        "mutual_friend_counts = mutual_friend.reduceByKey( lambda x,y : x+y)\n",
        "\n",
        "# Create the recommendation objects, group them by key, then sort and \n",
        "recommendations = mutual_friend_counts.flatMap(mutual_friend_count_to_recommendation).groupByKey() \n",
        "\n",
        "# Truncate the recommendations to the 10 most highly recommended.\n",
        "recommendations10 = recommendations.map(lambda m: (m[0], recommendation_to_sorted_truncated(list(m[1])))).sortByKey() \n",
        "\n",
        "# Include in your writeup the recommendations for the users with following user IDs: 924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993.\n",
        "results = recommendations10.filter(lambda recommendations: recommendations[0] in [924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MBAUooVewnGN",
        "outputId": "7518d78b-d6c7-43c7-8fe8-ac9cec7fdc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Tie2gijVwuLw",
        "outputId": "3517a8b1-111f-4422-88db-645158ed46e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(924, [439, 2409, 6995, 11860, 15416, 43748, 45881]),\n",
              " (8941, [8943, 8944, 8940]),\n",
              " (8942, [8939, 8940, 8943, 8944]),\n",
              " (9019, [9022, 317, 9023]),\n",
              " (9020, [9021, 9016, 9017, 9022, 317, 9023]),\n",
              " (9021, [9020, 9016, 9017, 9022, 317, 9023]),\n",
              " (9022, [9019, 9020, 9021, 317, 9016, 9017, 9023]),\n",
              " (9990, [13134, 13478, 13877, 34299, 34485, 34642, 37941]),\n",
              " (9992, [9987, 9989, 35667, 9991]),\n",
              " (9993, [9991, 13134, 13478, 13877, 34299, 34485, 34642, 37941])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 2)** \n"
      ],
      "metadata": {
        "id": "0U-dfsNtRn0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 2d)**"
      ],
      "metadata": {
        "id": "aenO2vRIUg-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load dataset and remove duplicates\n",
        "data = []\n",
        "with open('/content/browsing.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        items = set(line.strip().split())\n",
        "        data.append(list(items))"
      ],
      "metadata": {
        "id": "f9bVwzUxUfvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First pass to find frequent items\n",
        "min_support = 100\n",
        "item_counts = defaultdict(int)\n",
        "for basket in data:\n",
        "    for item in basket:\n",
        "        item_counts[item] += 1\n",
        "frequent_items = set(item for item, count in item_counts.items() if count >= min_support)"
      ],
      "metadata": {
        "id": "LBpkgkJnUpBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second pass to find frequent itemsets of size 2\n",
        "itemset_counts = defaultdict(int)\n",
        "for basket in data:\n",
        "    for itemset in combinations(basket, 2):\n",
        "        if set(itemset).issubset(frequent_items):\n",
        "            itemset_counts[itemset] += 1\n",
        "frequent_itemsets = set(itemset for itemset, count in itemset_counts.items() if count >= min_support)"
      ],
      "metadata": {
        "id": "VBQ0ols_UsYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate association rules for pairs of items\n",
        "rules = []\n",
        "for itemset in frequent_itemsets:\n",
        "    for item in itemset:\n",
        "        antecedent = frozenset([item])\n",
        "        consequent = frozenset(frozenset(itemset) - antecedent)\n",
        "        confidence = itemset_counts[itemset] / item_counts[item]\n",
        "        rules.append((antecedent, consequent, confidence))"
      ],
      "metadata": {
        "id": "ePl6-OLeUuft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort rules by confidence and print top 5\n",
        "top_rules = sorted(rules, key=lambda x: (-x[2], tuple(x[0])))\n",
        "for antecedent, consequent, confidence in top_rules[:5]:\n",
        "    print(f\"{tuple(antecedent)} -> {tuple(consequent)} : {confidence}\")"
      ],
      "metadata": {
        "id": "p-YhevPvUyWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 2e)**"
      ],
      "metadata": {
        "id": "hMAJQlBUU3bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load dataset and remove duplicates\n",
        "data = []\n",
        "with open('/content/browsing.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        items = set(line.strip().split())\n",
        "        data.append(list(items))"
      ],
      "metadata": {
        "id": "o3szEEswU7SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First pass to find frequent items\n",
        "min_support = 100\n",
        "item_counts = defaultdict(int)\n",
        "for basket in data:\n",
        "    for item in basket:\n",
        "        item_counts[item] += 1\n",
        "frequent_items = set(item for item, count in item_counts.items() if count >= min_support)"
      ],
      "metadata": {
        "id": "0uJudIkcVApB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second pass to find frequent itemsets of size 3\n",
        "itemset_counts = defaultdict(int)\n",
        "for basket in data:\n",
        "    for itemset in combinations(basket, 3):\n",
        "        if set(itemset).issubset(frequent_items):\n",
        "            itemset_counts[itemset] += 1\n",
        "frequent_itemsets = set(itemset for itemset, count in itemset_counts.items() if count >= min_support)"
      ],
      "metadata": {
        "id": "GzaNcCk7VDwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate association rules for pairs of items\n",
        "rules = []\n",
        "for itemset in frequent_itemsets:\n",
        "    for item in itemset:\n",
        "        antecedent = frozenset([item])\n",
        "        consequent = frozenset(frozenset(itemset) - antecedent)\n",
        "        confidence = itemset_counts[itemset] / item_counts[item]\n",
        "        rules.append((antecedent, consequent, confidence))"
      ],
      "metadata": {
        "id": "wvdauK0OVIze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort rules by confidence and print top 5\n",
        "top_rules = sorted(rules, key=lambda x: (-x[2], tuple(x[0])))\n",
        "for antecedent, consequent, confidence in top_rules[:5]:\n",
        "    print(f\"{tuple(antecedent)} -> {tuple(consequent)} : {confidence}\")"
      ],
      "metadata": {
        "id": "PCL_K7EfVJl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu 4"
      ],
      "metadata": {
        "id": "d-S5L6ls3yF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import pdb\n",
        "import unittest\n",
        "from PIL import Image\n",
        "\n",
        "# Finds the L1 distance between two vectors\n",
        "# u and v are 1-dimensional np.array objects\n",
        "# TODO: Implement this\n",
        "def l1(u, v):\n",
        "    return np.sum(np.abs(u-v))\n",
        "\n",
        "# Loads the data into a np array, where each row corresponds to\n",
        "# an image patch -- this step is sort of slow.\n",
        "# Each row in the data is an image, and there are 400 columns.\n",
        "def load_data(filename):\n",
        "    return np.genfromtxt(filename, delimiter=',')\n",
        "\n",
        "# Creates a hash function from a list of dimensions and thresholds.\n",
        "def create_function(dimensions, thresholds):\n",
        "    def f(v):\n",
        "        boolarray = [v[dimensions[i]] >= thresholds[i] for i in range(len(dimensions))]\n",
        "        return \"\".join(map(str, map(int, boolarray)))\n",
        "    return f\n",
        "\n",
        "# Creates the LSH functions (functions that compute L K-bit hash keys).\n",
        "# Each function selects k dimensions (i.e. column indices of the image matrix)\n",
        "# at random, and then chooses a random threshold for each dimension, between 0 and\n",
        "# 255.  For any image, if its value on a given dimension is greater than or equal to\n",
        "# the randomly chosen threshold, we set that bit to 1.  Each hash function returns\n",
        "# a length-k bit string of the form \"0101010001101001...\", and the L hash functions \n",
        "# will produce L such bit strings for each image.\n",
        "def create_functions(k, L, num_dimensions=400, min_threshold=0, max_threshold=255):\n",
        "    functions = []\n",
        "    for i in range(L):\n",
        "        dimensions = np.random.randint(low = 0, \n",
        "                                   high = num_dimensions,\n",
        "                                   size = k)\n",
        "        thresholds = np.random.randint(low = min_threshold, \n",
        "                                   high = max_threshold + 1, \n",
        "                                   size = k)\n",
        "\n",
        "        functions.append(create_function(dimensions, thresholds))\n",
        "    return functions\n",
        "\n",
        "# Hashes an individual vector (i.e. image).  This produces an array with L\n",
        "# entries, where each entry is a string of k bits.\n",
        "def hash_vector(functions, v):\n",
        "    return np.array([f(v) for f in functions])\n",
        "\n",
        "# Hashes the data in A, where each row is a datapoint, using the L\n",
        "# functions in \"functions.\"\n",
        "def hash_data(functions, A):\n",
        "    return np.array(list(map(lambda v: hash_vector(functions, v), A)))\n",
        "\n",
        "# Retrieve all of the points that hash to one of the same buckets \n",
        "# as the query point.  Do not do any random sampling (unlike what the first\n",
        "# part of this problem prescribes).\n",
        "# Don't retrieve a point if it is the same point as the query point.\n",
        "def get_candidates(hashed_A, hashed_point, query_index):\n",
        "    return filter(lambda i: i != query_index and \\\n",
        "        any(hashed_point == hashed_A[i]), range(len(hashed_A)))\n",
        "\n",
        "# Sets up the LSH.  You should try to call this function as few times as \n",
        "# possible, since it is expensive.\n",
        "# A: The dataset in which each row is an image patch.\n",
        "# Return the LSH functions and hashed data structure.\n",
        "def lsh_setup(A, k = 24, L = 10):\n",
        "    functions = create_functions(k = k, L = L)\n",
        "    hashed_A = hash_data(functions, A)\n",
        "    return (functions, hashed_A)\n",
        "\n",
        "# Run the entire LSH algorithm\n",
        "def lsh_search(A, hashed_A, functions, query_index, num_neighbors = 10):\n",
        "    hashed_point = hash_vector(functions, A[query_index, :])\n",
        "    candidate_row_nums = get_candidates(hashed_A, hashed_point, query_index)\n",
        "    \n",
        "    distances = map(lambda r: (r, l1(A[r], A[query_index])), candidate_row_nums)\n",
        "    best_neighbors = sorted(distances, key=lambda t: t[1])[:num_neighbors]\n",
        "\n",
        "    return [t[0] for t in best_neighbors]\n",
        "\n",
        "# Plots images at the specified rows and saves them each to files.\n",
        "def plot(A, row_nums, base_filename):\n",
        "    for row_num in row_nums:\n",
        "        patch = np.reshape(A[row_num, :], [20, 20])\n",
        "        im = Image.fromarray(patch)\n",
        "        if im.mode != 'RGB':\n",
        "            im = im.convert('RGB')\n",
        "        im.save(base_filename + \"-\" + str(row_num) + \".png\")\n",
        "\n",
        "# Finds the nearest neighbors to a given vector, using linear search.\n",
        "def linear_search(A, query_index, num_neighbors):\n",
        "    query_vec = A[query_index, :]\n",
        "    search_results_list = []\n",
        "    idx_list = filter(lambda x: x!=query_index, range(len(A)))\n",
        "    for idx in idx_list:\n",
        "        search_results_list.append((idx, l1(query_vec, A[idx, :])))\n",
        "    best_neighbors = sorted(search_results_list, key=lambda x: x[1])[:num_neighbors]\n",
        "    return [t[0] for t in best_neighbors]\n",
        "\n",
        "# TODO: Write a function that computes the error measure\n",
        "def compute_error_measure(A, hashed_A, functions, j_list, num_neighbors=3):\n",
        "    error_sum = 0\n",
        "    for j in j_list:\n",
        "        lsh_top_neighbors = lsh_search(A, hashed_A, functions, j, num_neighbors)\n",
        "        lin_top_neighbors = linear_search(A, j, num_neighbors)\n",
        "        error_sum += np.sum([l1(A[j, :], A[top_n, :]) for top_n in lsh_top_neighbors]) / \\\n",
        "                     np.sum([l1(A[j, :], A[top_n, :]) for top_n in lin_top_neighbors])\n",
        "    return error_sum / len(j_list)\n",
        "\n",
        "\n",
        "# TODO: Solve Problem 4\n",
        "def problem4(file_path):\n",
        "\n",
        "    # Load data and setup LSH\n",
        "    data = load_data(file_path)\n",
        "    h_funcs_og, h_data_og = lsh_setup(data)\n",
        "\n",
        "    # Average time for top 3 near neighbors\n",
        "    query_indices = range(100, 1001, 100)\n",
        "\n",
        "    time_vals = []\n",
        "    for query_index in query_indices:\n",
        "        t0 = time.time()\n",
        "        lsh_search(data, h_data_og, h_funcs_og, query_index, 3)\n",
        "        t1 = time.time()\n",
        "        time_vals.append(t1-t0)\n",
        "    print(f'Thời gian chạy trung bình của LSH: {np.mean(time_vals):.3f} giây')\n",
        "\n",
        "    time_vals = []\n",
        "    for query_index in query_indices:\n",
        "        t0 = time.time()\n",
        "        linear_search(data, query_index, 3)\n",
        "        t1 = time.time()\n",
        "        time_vals.append(t1-t0)\n",
        "    print(f'Thời gian chạy trung bình của linear search {np.mean(time_vals):.3f} giây')\n",
        "\n",
        "    # Plot the error value as a function of L:\n",
        "    l_vals = range(10, 21, 2)\n",
        "    e_vals = []\n",
        "    for l_val in l_vals:\n",
        "        h_funcs, h_data = lsh_setup(data, L=l_val, k=24)\n",
        "        e_vals.append(compute_error_measure(data, h_data, h_funcs, query_indices, 3))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(l_vals, e_vals)\n",
        "    ax.set(xlabel='L', ylabel='Error Measure', title='Error Measure as a Function of L')\n",
        "    fig.savefig(\"error_vs_L.png\")\n",
        "\n",
        "    # Plot the error value as a function of k:\n",
        "    k_vals = range(16, 25, 2)\n",
        "    e_vals = []\n",
        "    for k_val in k_vals:\n",
        "        h_funcs, h_data = lsh_setup(data, k=k_val, L=10)\n",
        "        e_vals.append(compute_error_measure(data, h_data, h_funcs, query_indices, 3))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(k_vals, e_vals)\n",
        "    ax.set(xlabel='k', ylabel='Error Measure', title='Error Measure as a Function of k')\n",
        "    fig.savefig(\"error_vs_k.png\")\n",
        "\n",
        "    # Plot top 10 neighbors found using both methods\n",
        "    plot(data, [100], 'query_img_')\n",
        "    plot(data, lsh_search(data, h_data_og, h_funcs_og, 100, 10), 'lsh_')\n",
        "    plot(data, linear_search(data, 100, 10), 'lin_')\n",
        "\n",
        "#### TESTS #####\n",
        "\n",
        "class TestLSH(unittest.TestCase):\n",
        "    def test_l1(self):\n",
        "        u = np.array([1, 2, 3, 4])\n",
        "        v = np.array([2, 3, 2, 3])\n",
        "        self.assertEqual(l1(u, v), 4)\n",
        "\n",
        "    def test_hash_data(self):\n",
        "        f1 = lambda v: sum(v)\n",
        "        f2 = lambda v: sum([x * x for x in v])\n",
        "        A = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "        self.assertEqual(f1(A[0,:]), 6)\n",
        "        self.assertEqual(f2(A[0,:]), 14)\n",
        "\n",
        "        functions = [f1, f2]\n",
        "        self.assertTrue(np.array_equal(hash_vector(functions, A[0, :]), np.array([6, 14])))\n",
        "        self.assertTrue(np.array_equal(hash_data(functions, A), np.array([[6, 14], [15, 77]])))\n",
        "\n",
        "    ### TODO: Write your tests here (they won't be graded, \n",
        "    def test_linear_search(self):\n",
        "        q_ind = 0\n",
        "        A = np.array([[0,0,0], [5,0,0], [6,0,0], [7,0,0], [7,5,5], [7,7,7]])\n",
        "        self.assertEqual(linear_search(A, q_ind, 3), [1,2,3])"
      ],
      "metadata": {
        "id": "Ps9qs8Su8Oxx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/patches.csv\"\n",
        "problem4(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zSobab_X8U3-",
        "outputId": "51712074-0403-4a54-9aaa-47d90c001f87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c5092c0204f8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/patches.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproblem4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-e6c32dbe9c71>\u001b[0m in \u001b[0;36mproblem4\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Load data and setup LSH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mh_funcs_og\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_data_og\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsh_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e6c32dbe9c71>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Each row in the data is an image, and there are 400 columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Creates a hash function from a list of dimensions and thresholds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   2143\u001b[0m             \u001b[0;31m# Raise an exception ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minvalid_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2146\u001b[0m             \u001b[0;31m# Issue a warning ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some errors were detected !\n    Line #105 (got 685 columns instead of 400)\n    Line #210 (got 58 columns instead of 400)\n    Line #315 (got 685 columns instead of 400)\n    Line #419 (got 458 columns instead of 400)\n    Line #524 (got 285 columns instead of 400)\n    Line #629 (got 458 columns instead of 400)\n    Line #734 (got 285 columns instead of 400)\n    Line #839 (got 458 columns instead of 400)\n    Line #944 (got 285 columns instead of 400)\n    Line #1049 (got 458 columns instead of 400)\n    Line #1154 (got 285 columns instead of 400)\n    Line #1259 (got 458 columns instead of 400)\n    Line #1364 (got 285 columns instead of 400)\n    Line #1469 (got 458 columns instead of 400)\n    Line #1573 (got 685 columns instead of 400)\n    Line #1678 (got 58 columns instead of 400)\n    Line #1783 (got 685 columns instead of 400)\n    Line #1887 (got 458 columns instead of 400)\n    Line #1992 (got 285 columns instead of 400)\n    Line #2097 (got 458 columns instead of 400)\n    Line #2202 (got 285 columns instead of 400)\n    Line #2307 (got 458 columns instead of 400)\n    Line #2412 (got 285 columns instead of 400)\n    Line #2517 (got 458 columns instead of 400)\n    Line #2622 (got 285 columns instead of 400)\n    Line #2727 (got 458 columns instead of 400)\n    Line #2832 (got 285 columns instead of 400)\n    Line #2937 (got 458 columns instead of 400)\n    Line #3041 (got 685 columns instead of 400)\n    Line #3146 (got 58 columns instead of 400)\n    Line #3251 (got 685 columns instead of 400)\n    Line #3355 (got 458 columns instead of 400)\n    Line #3460 (got 285 columns instead of 400)\n    Line #3565 (got 458 columns instead of 400)\n    Line #3670 (got 285 columns instead of 400)\n    Line #3775 (got 458 columns instead of 400)\n    Line #3880 (got 285 columns instead of 400)\n    Line #3985 (got 459 columns instead of 400)\n    Line #4090 (got 284 columns instead of 400)\n    Line #4195 (got 459 columns instead of 400)\n    Line #4300 (got 284 columns instead of 400)\n    Line #4405 (got 459 columns instead of 400)\n    Line #4509 (got 347 columns instead of 400)"
          ]
        }
      ]
    }
  ]
}